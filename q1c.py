# -*- coding: utf-8 -*-
"""Ass4q1c.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13TuOT2Oh-_RJdi3U7oO9du3V-cDS7h1N
"""



# from google.colab import drive
# drive.mount('/content/drive')

# ! cp "drive/My Drive/ass4_data.zip" ./

# ! unzip ass4_data.zip

# ! pip install torch

import torch
import numpy as np
import sys
def getData(xPath):
  x_in = np.genfromtxt(xPath ,delimiter=",")
  x = x_in[:,1:]
  y = x_in[:,0]
  # x = x/255
  return x,y

xTrain,yTrain = getData(sys.argv[1])
xTest,yTest = getData(sys.argv[2])
r = len(np.unique(yTrain))
n = np.shape(xTrain)[1]
numFeatures = n
m = np.shape(xTrain)[0]

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = torch.nn.Sequential(
        torch.nn.Conv2d(1, 64, kernel_size=3, stride=3, padding=0),
        torch.nn.BatchNorm2d(64),
        torch.nn.ReLU(),
        torch.nn.MaxPool2d(2, stride=2),
        torch.nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=0),
        torch.nn.BatchNorm2d(128),
        torch.nn.ReLU(),
        torch.nn.MaxPool2d(2, stride=2),
        torch.nn.Flatten(),
        torch.nn.Linear(in_features=512, out_features=256),
        torch.nn.BatchNorm1d(256),
        torch.nn.ReLU(),
        torch.nn.Linear(in_features=256, out_features=r)
    )

# print(model)

# model = model.cuda() ##Check whether cuda is available or not
xTrain = torch.tensor(xTrain,dtype=torch.float).to(device)
yTrain = torch.tensor(yTrain,dtype=torch.long).to(device)

optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # stochastic gradient descent
criterion = torch.nn.CrossEntropyLoss()

lossList = []
epochList = []

EPOCHS = 2000
M = 100
model = model.to(device)
c = 0
prevJ = 0
earlyStop = 0
epoch = 0
while epoch<100 and earlyStop<10:
    epoch+=1
    for i in range(0,int(m/M)):
      # optimizer.zero_grad()
      x_train = (xTrain[(i)*M:(i)*M + M,:])
      y_train = (yTrain[i*M:i*M + M])
      # x_train = x_train.unsqueeze(1)
      x_train = x_train.view(M,1,48,48)
      outputs = model(x_train)
      loss = criterion(outputs, y_train)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      if (i+1) % 50 == 0:
        lossList.append(loss.item())
        epochList.append(c)
        # print(abs(loss.item()-prevJ))
        # print ('Loss: {:.4f}'.format(loss.item()))
      if abs(loss.item()-prevJ) < 1e-5:
        earlyStop += 10
      prevJ = loss.item()

# print(epoch)

# with torch.no_grad():
#     # print("h")
#     correct = 0
#     total = 0
#     M = 100
#     # print(m/M)
#     for i in range(0,int(m/M)):
#       # print(i)
#       # optimizer.zero_grad()
#       x_train = (xTrain[(i)*M:(i)*M + M,:])
#       y_train = (yTrain[i*M:i*M + M])
#       # x_train = x_train.unsqueeze(1)
#       x_train = x_train.view(M,1,48,48)
#       outputs = model(x_train)
#       y_predicted = torch.argmax(outputs,dim=1)
#       # print(y_predicted)
#       # _, predicted = torch.max(outputs, 1)
#       total += x_train.size(0)
#       for i in range(len(y_predicted)):
#         if y_train[i] == y_predicted[i]:
#           correct += 1
    # print(correct,total)
    # print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))

xTest = torch.tensor(xTest,dtype=torch.float).to(device)
yTest = torch.tensor(yTest,dtype=torch.long).to(device)
xTest = xTest.view(xTest.size(0),1,48,48)
y_predicted = model(xTest)
# print(y_predicted)
y_predicted = torch.argmax(y_predicted,dim=1)
np.savetxt(sys.argv[3], y_predicted.cpu(), fmt="%d", delimiter="\n")

# # print(y_predicted)
# # print(yTrain)
# # print(yTrain.size())
# correct = 0
# for i in range(len(y_predicted)):
#   if yTest[i] == y_predicted[i]:
#     correct += 1
# # yTrain = yTrain.view(m,-1)
# # correct += (y_predicted == yTrain).float().sum()
# print(correct)
# accuracy = 100 * correct / ((xTest.size)(0))
# print("Accuracy = {}".format(accuracy))

# from torchsummary import summary
# summary(model, (1,48,48))

# confusion_matrix = torch.zeros(7, 7)
# y_predicted = model(xTest)
# print(y_predicted)
# y_predicted = torch.argmax(y_predicted,dim=1)
# # print(y_predicted)
# # print(yTrain)
# # print(yTrain.size())
# correct = 0
# for t, p in zip(yTest.view(-1), y_predicted.view(-1)):
#   confusion_matrix[t.long(), p.long()] += 1

# true_positive = torch.eq(yTest, y_predicted).sum().float()
# f1_score = torch.div(true_positive, len(yTest))

# print(confusion_matrix.int())

# print(f1_score)

